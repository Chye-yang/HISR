# HISR 核心代码详细注释说明

本文档基于《HISR构建报告V5》和项目README，对HISR核心模块进行详细注释说明。

---

## 目录

1. [encoder_bipartite.py - 二部图编码器](#encoder_bipartitepy)
2. [decoder_prefix_tree.py - 前缀树解码器](#decoder_prefix_treepy)
3. [prefix.py - 层次化键空间表示](#prefixpy)
4. [bucketize.py - 逻辑分桶模块](#bucketizepy)
5. [local_operator.py - 桶局部算子提取](#local_operatorpy)
6. [train_eval_hisr.py - 训练评估入口](#train_eval_hisrpy)

---

## encoder_bipartite.py

### 理论基础（对应HISR构建报告V5 Sec. 3.3.2）

**核心思想**: 将桶局部算子 A_{e,b} 建模为二部图 G_{e,b} = (U_b, V_{e,b}, E_{e,b})
- U_b: 桶内键节点集合
- V_{e,b}: 键连接的计数器节点集合
- E_{e,b}: 键到计数器的边（由CM-sketch哈希函数定义）

**编码过程**: T层交替消息传递
1. Counter→Key (C2K): 计数器向键聚合观测信号
2. Key→Counter (K2C): 键向计数器传播更新，迭代优化

**输出**: 解耦表示 (z_c, z_v)
- z_c: 环境不变表示 → 用于解码器，学习跨环境稳定的恢复机制
- z_v: 环境特定表示 → 捕获环境特异的碰撞模式等

### 类: BipartiteGNNEncoder

#### 初始化参数详解

```python
def __init__(
    self,
    *,
    d_node: int = 128,           # 节点嵌入维度
    d_msg: int = 128,            # 消息嵌入维度
    d_z: int = 128,              # 不变表征维度
    num_layers: int = 3,         # GNN层数T（交替消息传递轮数）
    use_gru: bool = True,        # 是否使用GRU门控更新
    dropout: float = 0.1,        # Dropout率
    max_counter_id_buckets: int = 1 << 20,  # 计数器ID哈希表大小
    max_key_id_buckets: int = 1 << 20,      # 键ID哈希表大小
)
```

#### 模块组件详解

**1. 结构ID嵌入**
```python
self.counter_emb = nn.Embedding(max_counter_id_buckets, d_node)
self.key_emb = nn.Embedding(max_key_id_buckets, d_node)
```
- **作用**: 为计数器和键提供结构化的初始嵌入
- **设计理由**: 
  - 计数器ID可能非常大（sketch宽度×深度），使用哈希映射到有限表
  - 保持跨桶的结构一致性，避免模型学习到桶编号等捷径
  - 支持OOD泛化（新环境下的计数器位置变化时，嵌入仍稳定）

**2. 计数器初始特征投影**
```python
self.counter_val_proj = nn.Sequential(
    nn.Linear(2, d_node),
    nn.ReLU(inplace=True),
)
```
- **输入**: 计数器观测值特征 [y, log(1+y)]
  - y: 计数器原始观测值
  - log(1+y): 对数变换，帮助处理长尾分布
- **作用**: 将观测值映射到嵌入空间，与ID嵌入相加

**3. 消息传递网络**
```python
self.msg_c2k = MLP(d_node, d_msg, d_node, dropout=dropout)  # Counter→Key
self.msg_k2c = MLP(d_node, d_msg, d_node, dropout=dropout)  # Key→Counter
```
- **C2K消息**: 从计数器观测中提取关于键频率的信息
- **K2C消息**: 从键推测中传播回计数器，帮助校准观测

**4. 节点状态更新函数**
```python
if self.use_gru:
    self.upd_key = nn.GRUCell(d_node, d_node)   # GRU门控更新
    self.upd_ctr = nn.GRUCell(d_node, d_node)
else:
    self.upd_key = MLP(2 * d_node, d_msg, d_node)  # 拼接旧状态和新消息
    self.upd_ctr = MLP(2 * d_node, d_msg, d_node)
```
- **GRU版本**: 门控机制，更好地建模长程依赖
- **MLP版本**: 简单拼接，计算开销更小

**5. 解耦投影头**
```python
self.proj_key_c = MLP(d_node, d_msg, d_node, dropout=dropout)  # 不变表示
self.proj_key_v = MLP(d_node, d_msg, d_node, dropout=dropout)  # 特异表示
```
- **z_c投影**: 学习跨环境稳定的恢复机制
  - 如: 基于图结构的信息传播规律
  - 如: 频率分布的先验知识
- **z_v投影**: 捕获环境特异信息
  - 如: 特定哈希种子下的碰撞模式
  - 如: 阶段性的流量偏置

**6. 桶级表征池化**
```python
self.pool_to_zc = MLP(d_node, d_msg, d_z, dropout=dropout)
self.pool_to_zv = MLP(d_node, d_msg, d_z, dropout=dropout)
```
- 将所有键的表示平均池化到桶级
- z_c用于解码器输入，z_v用于不变性约束（IRM）

#### 前向传播详解

```python
def forward(self, graph: BucketGraph, ...) -> EncoderOutput:
```

**步骤1: 初始化节点状态**
```python
# 键节点: 使用哈希后的ID嵌入
h_key = self.key_emb(key_ids_h)

# 计数器节点: ID嵌入 + 观测值特征
ctr_feat = torch.stack([y, torch.log1p(torch.clamp_min(y, 0.0))], dim=-1)
h_ctr = self.counter_emb(ctr_ids_h) + self.counter_val_proj(ctr_feat)
```

**步骤2: T层交替消息传递**
```python
for _ in range(self.num_layers):
    # ============================
    # Counter → Key (C2K)
    # ============================
    # 每个计数器向连接的键发送消息
    msg = self.msg_c2k(h_ctr[dst_c])  # (E, d)
    # 聚合: 每个键收到来自多个计数器的消息
    agg = torch.zeros((num_keys, self.d_node), device=device, dtype=h_key.dtype)
    agg.index_add_(0, src_k, msg)
    # 度归一化: 除以连接的计数器数量
    agg = agg / (deg.unsqueeze(-1).clamp_min(1.0))
    # 更新键状态
    h_key = self.upd_key(self.drop(agg), h_key)
    h_key = self.norm_key(h_key)
    
    # ============================
    # Key → Counter (K2C)
    # ============================
    # 每个键向连接的计数器发送推测信息
    msg2 = self.msg_k2c(h_key[src_k])
    # 聚合到计数器
    agg2 = torch.zeros((num_ctrs, self.d_node), device=device, dtype=h_ctr.dtype)
    agg2.index_add_(0, dst_c, msg2)
    # 度归一化
    agg2 = agg2 / (deg2.unsqueeze(-1).clamp_min(1.0))
    # 更新计数器状态
    h_ctr = self.upd_ctr(self.drop(agg2), h_ctr)
    h_ctr = self.norm_ctr(h_ctr)
```

**步骤3: 解耦表示提取**
```python
# 将键状态投影到不变/特异子空间
h_key_c = self.proj_key_c(h_key)
h_key_v = self.proj_key_v(h_key)

# 池化到桶级
pooled_c = h_key_c.mean(dim=0)  # 平均池化
pooled_v = h_key_v.mean(dim=0)

# 投影到最终表征
z_c = self.pool_to_zc(pooled_c)  # 用于解码
z_v = self.pool_to_zv(pooled_v)  # 用于IRM约束
```

---

## decoder_prefix_tree.py

### 理论基础（对应HISR构建报告V5 Sec. 3.3.3）

**核心思想**: 质量守恒的层次化解码
- 将桶内总质量（流量）按比例逐级分配给子节点
- 使用softmax归一化分裂比例，自动满足守恒约束
- 前缀树结构：root → /16 → /24 → /32(keys)

**守恒约束**:
```
c(parent) = Σ_{children} c(child)
```
通过分裂比例实现:
```
c(child) = α_{parent→child} × c(parent)
Σ α = 1
```

### 类: PrefixTreeDecoder

#### 初始化参数

```python
def __init__(
    self,
    *,
    d_z: int = 128,            # 不变表征维度（与编码器匹配）
    d_hidden: int = 128,       # 隐藏层维度
    dropout: float = 0.1,
    max_prefix_buckets: int = 1 << 20,  # 前缀ID哈希表大小
    max_level: int = 8,        # 最大层级数
)
```

#### 模块组件

**1. 根节点质量预测**
```python
self.root_head = nn.Sequential(
    nn.Linear(d_z, d_hidden),
    nn.ReLU(inplace=True),
    nn.Dropout(dropout),
    nn.Linear(d_hidden, 1),
)
```
- **输入**: 桶级不变表征 z_c
- **输出**: 根节点质量 c(root) = softplus(w^T z_c)
- **softplus**: 确保质量非负

**2. 子节点评分器**
```python
self.child_scorer = nn.Sequential(
    nn.Linear(d_z + d_hidden + d_hidden + 1, d_hidden),  # 输入维度
    nn.ReLU(inplace=True),
    nn.Dropout(dropout),
    nn.Linear(d_hidden, 1),  # 输出标量logit
)
```
- **输入特征**:
  1. z_c: 桶级不变表征（全局上下文）
  2. prefix_emb: 前缀嵌入（结构信息）
  3. level_emb: 层级嵌入（位置信息）
  4. log(size+1): 子节点覆盖的键数量（规模信息）
- **输出**: 子节点的标量分数（用于softmax归一化）

#### 前向传播详解

```python
def forward(self, z_c: torch.Tensor, tree: PrefixTreeSpec):
```

**步骤1: 预测根节点质量**
```python
c_root = torch.nn.functional.softplus(self.root_head(z_c)).squeeze(-1)
mass[tree.root.node_id] = c_root
```

**步骤2: 逐层分裂质量**
```python
for lvl in range(0, len(tree.nodes_by_level) - 1):  # 0→1→2→3
    next_lvl = lvl + 1
    
    for parent_node in lvl_nodes:
        parent_mass = mass[parent_node.node_id]
        child_ids = tree.children.get(parent_node.node_id, [])
        
        # 为每个子节点计算分数
        logits = []
        for cid in child_ids:
            child_node = tree.nodes_by_level[next_lvl][...]
            pe, le, size = self._node_feature(child_node)
            feat = torch.cat([z_c, pe, le, size], dim=0)
            logit = self.child_scorer(feat).squeeze(-1)
            logits.append(logit)
        
        # Softmax归一化得到分裂比例
        alpha = torch.softmax(torch.stack(logits, dim=0), dim=0)
        
        # 按比例分配父节点质量
        for a, cid in zip(alpha, child_ids):
            mass[cid] = a * parent_mass
```

**步骤3: 提取叶节点质量（最终预测）**
```python
x_hat = torch.zeros((tree.num_keys,), device=device)
for key_idx, leaf_id in enumerate(tree.leaf_of_key):
    x_hat[key_idx] = mass[leaf_id]
```

---

## prefix.py

### 理论基础

**核心概念**: Prefix-Scale多尺度建模
- 将"尺度"定义为键空间的前缀粒度
- IPv4地址: /16（粗）→ /24（中）→ /32（细）
- 通用u64键: 按位前缀定义层级

**Prefix ID计算**:
```
prefix_id = x >> (D - bits)
```
- x: 键的整数表示
- D: 域的比特数
- bits: 前缀长度

### 关键函数

**1. 前缀ID提取**
```python
def prefix_id(self, key: bytes, bits: int) -> int:
    D = self.domain_bits(key)
    x = self.key_to_int(key)
    return x >> (D - bits)
```

**2. 层级聚合**
```python
def aggregate_ground_truth_by_prefix(
    gt_key_freq: Mapping[bytes, int],
    hierarchy: PrefixHierarchy,
    level: int,
) -> Dict[int, int]:
    """
    将键级真值聚合到前缀级
    用于L1/L2层级的评测和热点检测
    """
```

**3. Top-K热点选择**
```python
def select_topk_prefixes(prefix_counts: Mapping[int, int], k: int):
    """选择Top-K流量最大的前缀（用于Zoom-in）"""
```

**4. 前缀过滤**
```python
def filter_keys_by_prefix(keys, hierarchy, level, allowed_prefixes):
    """
    保留属于特定前缀集合的键
    用于L2/L3阶段的候选键筛选
    """
```

---

## bucketize.py

### 理论基础

**目的**: 将候选键集划分为固定长度的逻辑桶
- 保持UCL-sketch的可扩展性接口
- 支持桶级并行解码
- 共享参数，独立推理

**策略**:
1. **sorted**: 按整数值排序后分块
2. **hash**: 稳定哈希后分块
3. **prefix**: 按前缀分组后分块（推荐，符合HISR Zoom-in思想）

### 关键类: BucketIndex

```python
@dataclass
class BucketIndex:
    bucket_len: int                    # 桶长度L
    buckets: List[List[bytes]]         # 桶列表
    key_to_pos: Dict[bytes, Tuple[int, int]]  # 键→(桶ID, 偏移)映射
```

---

## local_operator.py

### 理论基础

**目标**: 提取桶局部二部图 G_{e,b}
- 全局sketch: y ∈ R^{d×w}
- 桶局部: y_{e,b} ∈ R^{|V_{e,b}|}
- 局部算子: A_{e,b} ∈ R^{|V_{e,b}| × |U_b|} (稀疏)

### 关键函数

**1. 二部图提取**
```python
def extract_bucket_bipartite(
    *,
    cm_sketch,                    # CM-sketch对象
    keys_in_bucket,              # 桶内键列表
    device,
    counter_value_dtype,
) -> BucketGraph:
    """
    返回:
    - y: 桶内计数器观测值
    - edge_index: (2, E) 边索引 [key_id, counter_id]
    - num_keys, num_counters: 节点数量
    """
```

**2. 计数器预测**
```python
def predict_counters_from_x(x_hat: torch.Tensor, graph: BucketGraph):
    """
    计算重建的计数器值: ŷ = A x̂
    用于自监督损失: ||A x̂ - y||²
    """
```

---

## train_eval_hisr.py

### 理论基础（对应HISR构建报告V5）

**环境定义**: e = (τ, s, π)
- τ: 时间窗口（快照索引）
- s: 哈希视角（CM-sketch种子）
- π: 键空间阶段（前缀粒度焦点）

**训练目标**:
```
L = R_self_supervised + λ_irm × R_irm + λ_inv × R_inv
```

**1. 自监督风险**
```
R_self_supervised = E_e,b,τ[||A_{e,b} x̂_{e,b} - y_{e,b}||² + λ_sparse × Σ log(1+|x̂|)]
```

**2. IRM惩罚**
```
R_irm = Σ_e ||∇_w R_e||²
其中 R_e = R_self_supervised 在环境e上的值
w: 学习的标量缩放因子
```

**3. 不变性对齐**
```
R_inv = Var({z_c^{(e)}})  # 跨环境不变表征的方差
```

### 训练流程

```python
def train(self, envs, stage, enc, dec, w, opt):
    for step in range(train_steps):
        # 1. 采样环境、时间窗口、桶
        tau = random.randrange(T)
        b = random.randrange(num_buckets)
        
        risks = []
        zc_list = []
        
        # 2. 对每个环境计算风险
        for ev in envs:
            # 提取桶局部图
            graph = extract_bucket_bipartite(ev.sketch.cm, keys_b, device)
            
            # 编码
            out = enc(graph)
            
            # 解码
            x_hat, _ = dec(out.z_c, tree)
            x_hat = torch.relu(x_hat)  # 非负约束
            
            # IRM标量缩放
            x_hat_scaled = w * x_hat
            
            # 计算环境内风险
            r = measurement_loss(graph, x_hat_scaled) + \
                sparse_lambda * sparsity_loss(x_hat_scaled)
            risks.append(r)
            zc_list.append(out.z_c)
        
        # 3. 计算总损失
        risk_mean = torch.stack(risks).mean()
        penalty = irm_penalty(risks, w)
        inv_loss = invariant_alignment_loss(zc_list)
        
        loss = risk_mean + irm_lambda * penalty + inv_lambda * inv_loss
        
        # 4. 反向传播和优化
        opt.zero_grad()
        loss.backward()
        clip_grad_norm_(...)
        opt.step()
```

### 三阶段Zoom-in流程

```python
def build_stages(cm_keys, gt):
    # 阶段1: L1 - 粗粒度全扫描
    stage1 = StageSpec(
        name="L1_all",
        keys=cm_keys,
        bucket_index=build_buckets(cm_keys, bucket_len_stage1, "prefix")
    )
    
    # 识别热点/16
    gt_l1 = aggregate_ground_truth_by_prefix(gt, hierarchy, level=0)
    hot_l1 = select_topk_prefixes(gt_l1, topk_l1)
    keys_l2 = filter_keys_by_prefix(cm_keys, hierarchy, 0, hot_l1)
    
    # 阶段2: L2 - 在热点/16内细化
    stage2 = StageSpec(
        name="L2_hot16",
        keys=keys_l2,
        bucket_index=build_buckets(keys_l2, bucket_len_stage2, "prefix")
    )
    
    # 识别热点/24
    gt_l2 = aggregate_ground_truth_by_prefix({k:gt[k] for k in keys_l2}, hierarchy, 1)
    hot_l2 = select_topk_prefixes(gt_l2, topk_l2)
    keys_l3 = filter_keys_by_prefix(keys_l2, hierarchy, 1, hot_l2)
    
    # 阶段3: L3 - 在热点/24内精确恢复
    stage3 = StageSpec(
        name="L3_hot24",
        keys=keys_l3,
        bucket_index=build_buckets(keys_l3, bucket_len_stage3, "sorted")
    )
    
    return [stage1, stage2, stage3]
```

---

## 总结

HISR的核心创新点在代码中的体现：

1. **Prefix-Scale多尺度**: prefix.py + decoder_prefix_tree.py
   - 层级化前缀定义 (/16→/24→/32)
   - 前缀树结构支持逐级Zoom-in

2. **环境不变性学习**: encoder_bipartite.py + train_eval_hisr.py
   - 解耦表示 (z_c, z_v)
   - IRM惩罚项强制跨环境一致性

3. **热点聚焦机制**: bucketize.py + train_eval_hisr.py
   - 三阶段训练 (L1/L2/L3)
   - Top-K热点选择和候选键过滤

4. **质量守恒解码**: decoder_prefix_tree.py
   - 前缀树分裂比例
   - 自动满足守恒约束

5. **二部图GNN**: encoder_bipartite.py
   - Counter↔Key交替消息传递
   - 哈希ID嵌入避免捷径学习

这些模块协同工作，实现了在高噪声sketch观测下的精确频率估计，并在OOD场景下保持鲁棒性。